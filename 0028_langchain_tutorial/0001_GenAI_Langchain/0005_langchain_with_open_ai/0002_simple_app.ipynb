{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Gen AI APP Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x187103ef490>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nUse monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonSearchRegionUSEUGo to AppQuick StartObservabilityTutorialsAdd observability to your LLM applicationHow-to GuidesTracingMonitoring and automationsFilter traces in the applicationUse monitoring chartsDashboardsSet up automation rulesOnline EvaluationSet up threadsSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesMonitoring and automationsUse monitoring chartsOn this pageUse monitoring charts\\nLangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.\\nChange the time period\\u200b\\nYou can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days.\\nSlice data by metadata or tag\\u200b\\nBy default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.\\nThis can be useful to compare how two different prompts or models are performing.\\nIn order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.\\nAfter that, you can click the Tag or Metadata tab at the top to group runs accordingly.\\n\\nDrill down into specific subsets\\u200b\\nMonitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard.\\nFrom there, you will be brought back to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into.\\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousFilter traces in the applicationNextDashboardsChange the time periodSlice data by metadata or tagDrill down into specific subsetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data--> Docs-->Divide our Docuemnts into chunks dcouments-->text-->vectors-->Vector Embeddings--->Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='Skip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonSearchRegionUSEUGo to AppQuick StartObservabilityTutorialsAdd observability to your LLM applicationHow-to GuidesTracingMonitoring and automationsFilter traces in the applicationUse monitoring chartsDashboardsSet up automation rulesOnline EvaluationSet up threadsSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesMonitoring and automationsUse monitoring chartsOn this pageUse monitoring charts\\nLangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.\\nChange the time period\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.\\nChange the time period\\u200b\\nYou can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days.\\nSlice data by metadata or tag\\u200b\\nBy default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.\\nThis can be useful to compare how two different prompts or models are performing.\\nIn order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.\\nAfter that, you can click the Tag or Metadata tab at the top to group runs accordingly.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='Drill down into specific subsets\\u200b\\nMonitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard.\\nFrom there, you will be brought back to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into.\\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousFilter traces in the applicationNextDashboardsChange the time periodSlice data by metadata or tagDrill down into specific subsetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1876ffb4810>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.\\nChange the time period\\u200b\\nYou can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days.\\nSlice data by metadata or tag\\u200b\\nBy default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.\\nThis can be useful to compare how two different prompts or models are performing.\\nIn order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.\\nAfter that, you can click the Tag or Metadata tab at the top to group runs accordingly.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query=\"You can view monitors over differing time periods.\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000018710351CD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000187634D36D0>, root_client=<openai.OpenAI object at 0x000001876CECD8D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000018762BA49D0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By default, monitors are set to a time period of seven days. You can change the time period by using the tabs at the top of the page.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"You can view monitors over differing time periods.\",\n",
    "    \"context\":[Document(page_content=\"You can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1876ffb4810>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input--->Retriever--->vectorstoredb\n",
    "\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001876FFB4810>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000018710351CD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000187634D36D0>, root_client=<openai.OpenAI object at 0x000001876CECD8D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000018762BA49D0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How can you compare the performance of two different prompts or models in LangSmith's monitoring charts?\\n\\nTo compare the performance of two different prompts or models in LangSmith's monitoring charts, you can slice the data by metadata or tags. First, ensure that you are attaching appropriate tags or metadata to your runs when logging them. Then, use the Tag or Metadata tab at the top of the Monitor page to group and view specific subsets of runs accordingly.\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response form the LLM\n",
    "response=retrieval_chain.invoke({\"input\":\"How can I view monitors over differing time periods?\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How can I view monitors over differing time periods?',\n",
       " 'context': [Document(id='b8c7b049-372b-4bab-a5c4-085b070f6672', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.\\nChange the time period\\u200b\\nYou can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days.\\nSlice data by metadata or tag\\u200b\\nBy default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.\\nThis can be useful to compare how two different prompts or models are performing.\\nIn order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.\\nAfter that, you can click the Tag or Metadata tab at the top to group runs accordingly.'),\n",
       "  Document(id='6c929043-6c8e-4b2d-81f9-f047eb157f1a', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='Drill down into specific subsets\\u200b\\nMonitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard.\\nFrom there, you will be brought back to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into.\\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousFilter traces in the applicationNextDashboardsChange the time periodSlice data by metadata or tagDrill down into specific subsetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.'),\n",
       "  Document(id='1caac51e-e4b1-4b3e-9c70-320bd991e55a', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='Skip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonSearchRegionUSEUGo to AppQuick StartObservabilityTutorialsAdd observability to your LLM applicationHow-to GuidesTracingMonitoring and automationsFilter traces in the applicationUse monitoring chartsDashboardsSet up automation rulesOnline EvaluationSet up threadsSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesMonitoring and automationsUse monitoring chartsOn this pageUse monitoring charts\\nLangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.\\nChange the time period\\u200b'),\n",
       "  Document(id='588d34bd-c6c2-4dc9-bfaa-87205e0bbbcc', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith')],\n",
       " 'answer': \"How can you compare the performance of two different prompts or models in LangSmith's monitoring charts?\\n\\nTo compare the performance of two different prompts or models in LangSmith's monitoring charts, you can slice the data by metadata or tags. First, ensure that you are attaching appropriate tags or metadata to your runs when logging them. Then, use the Tag or Metadata tab at the top of the Monitor page to group and view specific subsets of runs accordingly.\"}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b8c7b049-372b-4bab-a5c4-085b070f6672', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.\\nChange the time period\\u200b\\nYou can view monitors over differing time periods. This can be controlled by the tabs at the top of the page. By default, it is set to seven days.\\nSlice data by metadata or tag\\u200b\\nBy default, the monitor tab shows results for all runs. However, you can slice the data by metadata or tags to view specific subsets of runs.\\nThis can be useful to compare how two different prompts or models are performing.\\nIn order to do this, you first need to make sure you are attaching appropriate tags or metadata to these runs when logging them.\\nAfter that, you can click the Tag or Metadata tab at the top to group runs accordingly.'),\n",
       " Document(id='6c929043-6c8e-4b2d-81f9-f047eb157f1a', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='Drill down into specific subsets\\u200b\\nMonitoring charts can be useful to idea when spikes in errors or latency may be occurring. When you observe those spikes in a monitoring dashboard, you can easily drill into the runs causing those issues by clicking on the dot in the dashboard.\\nFrom there, you will be brought back to the Traces tab, with a filter applied so you are only viewing the runs that occurred in the time bucket that you clicked into.\\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousFilter traces in the applicationNextDashboardsChange the time periodSlice data by metadata or tagDrill down into specific subsetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(id='1caac51e-e4b1-4b3e-9c70-320bd991e55a', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='Skip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonSearchRegionUSEUGo to AppQuick StartObservabilityTutorialsAdd observability to your LLM applicationHow-to GuidesTracingMonitoring and automationsFilter traces in the applicationUse monitoring chartsDashboardsSet up automation rulesOnline EvaluationSet up threadsSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesMonitoring and automationsUse monitoring chartsOn this pageUse monitoring charts\\nLangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.\\nChange the time period\\u200b'),\n",
       " Document(id='588d34bd-c6c2-4dc9-bfaa-87205e0bbbcc', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/monitoring/use_monitoring_charts', 'title': 'Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith has a collection of monitoring charts accessible for each tracing project. These can be accessed on the Monitor tab within a particular project.', 'language': 'en'}, page_content='Use monitoring charts | ü¶úÔ∏èüõ†Ô∏è LangSmith')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
