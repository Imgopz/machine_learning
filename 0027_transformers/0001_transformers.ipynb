{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What and why Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural Language Processing (NLP), \"transformers\" refer to a powerful deep learning architecture that utilizes a *\"multi-head attention\"* mechanism to analyze relationships between words in a sentence, allowing for highly accurate results on various NLP tasks like *machine translation*, *sentiment analysis*, and *question answering*, making it a leading approach in modern NLP due to its ability to capture complex contextual relationships between words within a sequence, significantly surpassing older models like RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parallely send all words to encoder\n",
    "Attention mechanism solved the problems with longer sentences, but still we are sending words one by one that is each time we are sending only one word at a time. We are still not able to send all the words at once.\n",
    "\n",
    "Not scalable -> If the document is huge we need to send one word at time is huge time complexity.\n",
    "\n",
    "So we need different architecture to handle this problem.\n",
    "\n",
    "#### Transfomers\n",
    "\n",
    "Transformers are not using LSTM RNN, instead they used Self Attention Module.\n",
    "\n",
    "All the words can be parallely sent to the encoder. (Positional Encoding -> an important concept)\n",
    "\n",
    "Hence it is scalable. It means we can train this model with huge data in lesser time when compared to RNN.\n",
    "\n",
    "GPT, BERT are examples of transformers, using the -*transfer*- learning concept we can create SOTA *(State of the Art)* models on top of the tranformers, since those transformers are trained with huge amout of data.\n",
    "\n",
    "### 2. Contextual Embedding\n",
    "*Contextual embedding*: It's a kind of embedding process where each word will have some sort of relation ship with other words in the sentences.\n",
    "\n",
    "Example: I'm Krishna and I love to play Cricket. \n",
    "\n",
    "In the above sentence the \"I\" is related to the name \"Krishna\" -> this relationship will be considered in the contextual embedding. And this can be achieved by self attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers follows Encoder - Decoder Architecture. And There are multiple encoders and decoders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://arxiv.org/html/1706.03762v7 - Attention Is All You Need\n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. High Level Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"high_level_1.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"high_level_1.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"high_level_2.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"high_level_2.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"transformer_arch.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"transformer_arch.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Encoders & Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"whatisinside_encoder_decoder.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"whatisinside_encoder_decoder.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"encoder.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"encoder.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here z1, z2, & z3 are contextual vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"encoder1.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"encoder1.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward Neural Network (ANN) converting z1, z2, & z3 vectors to different vectors and sending to next encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"self_attention_1.png\" width=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"self_attention_1.png\", width=200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention, also known as scaled dot-product attention, is a crucial mechanism in the transformer architecture that allows the model to weigh the importance of difference tokens in the input sequence relative to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention layer basically converting *<strong>embedded vectors</strong>* into *<strong>contextual embedding</strong>* vectors using Scaled Dot-Product Attention method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Inputs</u>: Queries, Keys, & Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<u><font color='#FFA500'>Query Vector</font></u> (Q): Query vectors represent the tokne for which we are calculating the attention. They help determine the importance of the other tokens in the context of the current token.\n",
    "\n",
    "<b>Importance:</b>\n",
    "\n",
    "<font color='#FFA500'><u>Focus Determination</u></font>: Queries help the model to decide which parts of the sequence to focus on for each specific token. By calculating the dot product between a query vector and all key vectors, the model assesses how much attention to give to each token relative to the current token.\n",
    "\n",
    "<u><font color='#FFA500'>Contextual Understanding</font></u>: Queries contribute to understanding the relationship between the current token and the rest of the sequence, which is essential for capturing dependencies and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><font color='#FFA500'>Key Vector</font></u> (K): Key vectors represent all the tokens in the sequence and are used to compare with the query vectors to calculate attention scores.\n",
    "\n",
    "<b>Importance:</b>\n",
    "\n",
    "<font color='#FFA500'><u>Relevance Measurement</u></font>: Keys are compared with queris to measure the relevance or compatibility of each token with the current token.  This comparison helps in determining how much attention each token should receive.\n",
    "\n",
    "<u><font color='#FFA500'>Information Retrieval</font></u>: Keys play a critical role inretrieving the most relevant information from the sequence by providing a basis for the attention mechanism to compute similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><font color='#FFA500'>Value Vector</font></u> (V): Value vectors hold the actual information that will be aggregated to form the output of the attention mechanism.\n",
    "\n",
    "<b>Importance:</b>\n",
    "\n",
    "<font color='#FFA500'><u>Information Aggregation</u></font>: Values contain the data that will be weighted by the attention scores.  The weighted sum of values forms the output of the self-attention mechanism, which is then passed on to the next layers in the network.\n",
    "\n",
    "<u><font color='#FFA500'>Context Preservation</font></u>: By weighting the values according to the attention scores, the model preserves and aggregates relevant context from the entire sequence, which is crucial for tasks like translation, summarization, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Token Embedding\n",
    "2. Linear Transformation\n",
    "\n",
    "    We create Q, K, & V by multiplying the embeddings by learned weights matrices $ W_Q $, $ W_K $, & $ W_V $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize some weight matrics based on the embedding matrix size and calculate Q, K, & V vectors by doing dot matrix operations. In this case we initialized weights as Identity matrix for our understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"linear_transformation.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"linear_transformation.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute Attention Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"compute_att_score.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"compute_att_score.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Scaling\n",
    "\n",
    "    We scale down the socres by dividing the dimensions of the key vector $ \\sqrt{d_K} $. Scaling in the attention mechanism is crucial to prevent the dot product from growing too large. <font color=\"orange\">To ensure stable gradients during training.</font>\n",
    "\n",
    "    Two Problems: 1. Gradient Exploading, 2. Softmax Saturation (vanishing gradient problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"without_scaling.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"without_scaling.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the most of the attention weight is assigned to the first key vector, and the second vector with very little. This leads to Softmax saturation, that is the weights are not going to be updated during backpropagation, (vanishing gradient problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"with_Scaling.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"with_Scaling.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Importance of Scaling:\n",
    "\n",
    "<b>Stabilizing Training</b>: Scaling prevents extreamly large dot products, which helps in stabilizing the gradients during backpropagation, making the training process more stable and efficient.\n",
    "\n",
    "<b>Preventing Saturation</b>: By sclaing the dot products, the softmax function produces more balanced attention weights, preventing the model from focusing too heavily on a single token and ignoring others. \n",
    "\n",
    "<b>Improved Learning</b>: Balanced attention weights enable the model to learn better representations by considering multiple relevant tokens in the sequence, leading to better performance on the tasks that require context understanding.\n",
    "\n",
    "<b>Scaling ensures that the dor products are kept within a range that allows the softmax funtion to operate effectively, providing a more balanced distribution of attention weights and improving the overall learning process of the model.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"scaling.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"scaling.png\", width=500) \n",
    "\n",
    "# here we only showed for the word, wiht respect to other words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Applying Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"apply_softmax.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"apply_softmax.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Weighted sum of values\n",
    "    \n",
    "    We multiply the attention weights by corresponding value vecotrs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"weighted_sum.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"weighted_sum.png\", width=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"summary_self_attention.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"summary_self_attention.png\", width=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"sa_one.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"sa_one.png\", width=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"sa_2.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"sa_2.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"mha_one.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"mha_one.png\", width=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"mha_two.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"mha_two.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Feed Forward Neural Network with Multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"feed_forward_prep.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"feed_forward_prep.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing#scrollTo=twSVFOM9SopW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Positional Encoding\n",
    "\n",
    "Representing the order of the sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding index at the end of each word vector will be an issue incase of bigger numbers. Instead adding a positional vector of the same length of the word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"positional_encoding_one.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"positional_encoding_one.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Positional Encoding:\n",
    "\n",
    "1. Sinusoidal Positional Encoding\n",
    "2. Learned Positional Encoding - Postional encodings are learned during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sinusoidal Positional Encoding:</b>  It uses sin and consine functions of different frequencies to create positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pe_formula.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"pe_formula.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where,\n",
    "\n",
    "* $pos$ is the position\n",
    "\n",
    "* $i$ is the dimension\n",
    "\n",
    "* $d_\\text{model}$ is the dimesionality of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"positional_encoding.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"positional_encoding.png\", width=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pe_one.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"pe_one.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization: 1. Batch, 2. Layer\n",
    "\n",
    "Doing normalize on each output vectors, $Z_1$,$Z_2$....$Z_n$ is called as Batch Normalization\n",
    "\n",
    "Doing Normalize on each layers of $Z_1$,$Z_2$....$Z_n$ vectors is called Layer Normalization, we calculate, z score\n",
    "after calculating $\\sigma$ & $\\mu$ for each layer\n",
    "\n",
    "In transformers we apply layer normalization, if all are zero in that specific layer its going to affect the learning. \n",
    "\n",
    "$\\gamma$ & $\\beta$ - Learnable parameters -> we can use this parameters if we don't want to normlize the final Z vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"learnable_params.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"learnable_params.png\", width=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"add_and_normalize.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"add_and_normalize.png\", width=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"normalize_one.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"normalize_one.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Encoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"encoder-decoer-arch.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"encoder-decoer-arch.png\", width=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"encoder_arch.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"encoder_arch.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Residuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"residuals.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"residuals.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Feed Forward Neural Net?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ffnn.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"ffnn.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Decoder in Tranfomers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer decoder is responsible for generating the output sequence one token at a time, using enocder's output and the previously generated tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Masked Multi Head Self Attention,\n",
    "2. Multi Head Attention (Encoder Decoder Attention)\n",
    "3. Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs are sent at single go, but he output is generated one word at a time.\n",
    "\n",
    "1. Training Mechanism\n",
    "2. Inference Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"masked_multi_head_one.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"masked_multi_head_one.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Masked Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Look Ahead Mask\n",
    "2. Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"masked_multi_head_two.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"masked_multi_head_two.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"masked_multi_head_3.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"masked_multi_head_3.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps manage the structure of the sequences being procesed and ensures the modle behaves correctly during training and inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason,\n",
    "\n",
    "Handling variable length sequences with padding MASK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose,\n",
    "\n",
    "1. To handle sequences of different length in batch.\n",
    "2. To ensure that padding tokens wihch are added to make sequences of uniform length, do not affect the model prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"padding_mask.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"padding_mask.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look Ahead Masking -> Maintain Auto Regressive Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that each position in the decoder output sequence can only attend to the previous position, but no future position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"combined_mask_1.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"combined_mask_1.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"combined_mask_2.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"combined_mask_2.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"masked_score.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"masked_score.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"masking.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"masking.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Encoder-Decoder Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ed-multihead-attention.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"ed-multihead-attention.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"ed-multihead.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"ed-multihead.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Final Decoder Linear and Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"final.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"final.png\", width=700)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"softmax.png \" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"softmax.png \", width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"target_trained.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"target_trained.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: cross validation). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
