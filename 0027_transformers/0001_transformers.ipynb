{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What and why Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural Language Processing (NLP), \"transformers\" refer to a powerful deep learning architecture that utilizes a *\"multi-head attention\"* mechanism to analyze relationships between words in a sentence, allowing for highly accurate results on various NLP tasks like *machine translation*, *sentiment analysis*, and *question answering*, making it a leading approach in modern NLP due to its ability to capture complex contextual relationships between words within a sequence, significantly surpassing older models like RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parallely send all words to encoder\n",
    "Attention mechanism solved the problems with longer sentences, but still we are sending words one by one that is each time we are sending only one word at a time. We are still not able to send all the words at once.\n",
    "\n",
    "Not scalable -> If the document is huge we need to send one word at time is huge time complexity.\n",
    "\n",
    "So we need different architecture to handle this problem.\n",
    "\n",
    "#### Transfomers\n",
    "\n",
    "Transformers are not using LSTM RNN, instead they used Self Attention Module.\n",
    "\n",
    "All the words can be parallely sent to the encoder. (Positional Encoding -> an important concept)\n",
    "\n",
    "Hence it is scalable. It means we can train this model with huge data in lesser time when compared to RNN.\n",
    "\n",
    "GPT, BERT are examples of transformers, using the -*transfer*- learning concept we can create SOTA *(State of the Art)* models on top of the tranformers, since those transformers are trained with huge amout of data.\n",
    "\n",
    "### 2. Contextual Embedding\n",
    "*Contextual embedding*: It's a kind of embedding process where each word will have some sort of relation ship with other words in the sentences.\n",
    "\n",
    "Example: I'm Krishna and I love to play Cricket. \n",
    "\n",
    "In the above sentence the \"I\" is related to the name \"Krishna\" -> this relationship will be considered in the contextual embedding. And this can be achieved by self attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers follows Encoder - Decoder Architecture. And There are multiple encoders and decoders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://arxiv.org/html/1706.03762v7 - Attention Is All You Need\n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. High Level Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"high_level_1.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"high_level_1.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"high_level_2.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"high_level_2.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"transformer_arch.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"transformer_arch.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Encoders & Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"whatisinside_encoder_decoder.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"whatisinside_encoder_decoder.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"encoder.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"encoder.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here z1, z2, & z3 are contextual vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"encoder1.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"encoder1.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward Neural Network (ANN) converting z1, z2, & z3 vectors to different vectors and sending to next encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"self_attention_1.png\" width=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"self_attention_1.png\", width=200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention, also known as scaled dot-product attention, is a crucial mechanism in the transformer architecture that allows the model to weigh the importance of difference tokens in the input sequence relative to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention layer basically converting *<strong>embedded vectors</strong>* into *<strong>contextual embedding</strong>* vectors using Scaled Dot-Product Attention method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Inputs</u>: Queries, Keys, & Values\n",
    "\n",
    "<u><font color='#FFA500'>Query Vector</font></u> (Q): Query vectors represent the tokne for which we are calculating the attention. They help determine the importance of the other tokens in the context of the current token.\n",
    "\n",
    "<u><font color='#FFA500'>Importance</font></u>: Queries help the model to decide which parts of the sequence to focus on for each specific token. By calculating the dot product between a query vector and all key vectors, the model assesses how much attention to give to each token relative to the current token.\n",
    "\n",
    "<u><font color='#FFA500'>Contextual Understanding</font></u>: Queries contribute to understanding the relationship between the current token and the rest of the sequence, which is essential for capturing dependencies and context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
