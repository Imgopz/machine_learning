{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What and why Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural Language Processing (NLP), \"transformers\" refer to a powerful deep learning architecture that utilizes a *\"multi-head attention\"* mechanism to analyze relationships between words in a sentence, allowing for highly accurate results on various NLP tasks like *machine translation*, *sentiment analysis*, and *question answering*, making it a leading approach in modern NLP due to its ability to capture complex contextual relationships between words within a sequence, significantly surpassing older models like RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parallely send all words to encoder\n",
    "Attention mechanism solved the problems with longer sentences, but still we are sending words one by one that is each time we are sending only one word at a time. We are still not able to send all the words at once.\n",
    "\n",
    "Not scalable -> If the document is huge we need to send one word at time is huge time complexity.\n",
    "\n",
    "So we need different architecture to handle this problem.\n",
    "\n",
    "#### Transfomers\n",
    "\n",
    "Transformers are not using LSTM RNN, instead they used Self Attention Module.\n",
    "\n",
    "All the words can be parallely sent to the encoder. (Positional Encoding -> an important concept)\n",
    "\n",
    "Hence it is scalable. It means we can train this model with huge data in lesser time when compared to RNN.\n",
    "\n",
    "GPT, BERT are examples of transformers, using the -*transfer*- learning concept we can create SOTA *(State of the Art)* models on top of the tranformers, since those transformers are trained with huge amout of data.\n",
    "\n",
    "### 2. Contextual Embedding\n",
    "*Contextual embedding*: It's a kind of embedding process where each word will have some sort of relation ship with other words in the sentences.\n",
    "\n",
    "Example: I'm Krishna and I love to play Cricket. \n",
    "\n",
    "In the above sentence the \"I\" is related to the name \"Krishna\" -> this relationship will be considered in the contextual embedding. And this can be achieved by self attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers follows Encoder - Decoder Architecture. And There are multiple encoders and decoders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://arxiv.org/html/1706.03762v7 - Attention Is All You Need\n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. High Level Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"high_level_1.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"high_level_1.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"high_level_2.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"high_level_2.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"transformer_arch.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"transformer_arch.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Encoders & Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"whatisinside_encoder_decoder.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"whatisinside_encoder_decoder.png\", width=700) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"encoder.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"encoder.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here z1, z2, & z3 are contextual vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"encoder1.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"encoder1.png\", width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward Neural Network (ANN) converting z1, z2, & z3 vectors to different vectors and sending to next encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"self_attention_1.png\" width=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"self_attention_1.png\", width=200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention, also known as scaled dot-product attention, is a crucial mechanism in the transformer architecture that allows the model to weigh the importance of difference tokens in the input sequence relative to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention layer basically converting *<strong>embedded vectors</strong>* into *<strong>contextual embedding</strong>* vectors using Scaled Dot-Product Attention method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Inputs</u>: Queries, Keys, & Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<u><font color='#FFA500'>Query Vector</font></u> (Q): Query vectors represent the tokne for which we are calculating the attention. They help determine the importance of the other tokens in the context of the current token.\n",
    "\n",
    "<b>Importance:</b>\n",
    "\n",
    "<font color='#FFA500'><u>Focus Determination</u></font>: Queries help the model to decide which parts of the sequence to focus on for each specific token. By calculating the dot product between a query vector and all key vectors, the model assesses how much attention to give to each token relative to the current token.\n",
    "\n",
    "<u><font color='#FFA500'>Contextual Understanding</font></u>: Queries contribute to understanding the relationship between the current token and the rest of the sequence, which is essential for capturing dependencies and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><font color='#FFA500'>Key Vector</font></u> (K): Key vectors represent all the tokens in the sequence and are used to compare with the query vectors to calculate attention scores.\n",
    "\n",
    "<b>Importance:</b>\n",
    "\n",
    "<font color='#FFA500'><u>Relevance Measurement</u></font>: Keys are compared with queris to measure the relevance or compatibility of each token with the current token.  This comparison helps in determining how much attention each token should receive.\n",
    "\n",
    "<u><font color='#FFA500'>Information Retrieval</font></u>: Keys play a critical role inretrieving the most relevant information from the sequence by providing a basis for the attention mechanism to compute similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><font color='#FFA500'>Value Vector</font></u> (V): Value vectors hold the actual information that will be aggregated to form the output of the attention mechanism.\n",
    "\n",
    "<b>Importance:</b>\n",
    "\n",
    "<font color='#FFA500'><u>Information Aggregation</u></font>: Values contain the data that will be weighted by the attention scores.  The weighted sum of values forms the output of the self-attention mechanism, which is then passed on to the next layers in the network.\n",
    "\n",
    "<u><font color='#FFA500'>Context Preservation</font></u>: By weighting the values according to the attention scores, the model preserves and aggregates relevant context from the entire sequence, which is crucial for tasks like translation, summarization, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Token Embedding\n",
    "2. Linear Transformation\n",
    "\n",
    "    We create Q, K, & V by multiplying the embeddings by learned weights matrices $ W_Q $, $ W_K $, & $ W_V $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize some weight matrics based on the embedding matrix size and calculate Q, K, & V vectors by doing dot matrix operations. In this case we initialized weights as Identity matrix for our understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"linear_transformation.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"linear_transformation.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute Attention Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"compute_att_score.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"compute_att_score.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Scaling\n",
    "\n",
    "    We scale down the socres by dividing the dimensions of the key vector $ \\sqrt{d_K} $. Scaling in the attention mechanism is crucial to prevent the dot product from growing too large. <font color=\"orange\">To ensure stable gradients during training.</font>\n",
    "\n",
    "    Two Problems: 1. Gradient Exploading, 2. Softmax Saturation (vanishing gradient problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"without_scaling.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"without_scaling.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the most of the attention weight is assigned to the first key vector, and the second vector with very little. This leads to Softmax saturation, that is the weights are not going to be updated during backpropagation, (vanishing gradient problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"with_Scaling.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"with_Scaling.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Importance of Scaling:\n",
    "\n",
    "<b>Stabilizing Training</b>: Scaling prevents extreamly large dot products, which helps in stabilizing the gradients during backpropagation, making the training process more stable and efficient.\n",
    "\n",
    "<b>Preventing Saturation</b>: By sclaing the dot products, the softmax function produces more balanced attention weights, preventing the model from focusing too heavily on a single token and ignoring others. \n",
    "\n",
    "<b>Improved Learning</b>: Balanced attention weights enable the model to learn better representations by considering multiple relevant tokens in the sequence, leading to better performance on the tasks that require context understanding.\n",
    "\n",
    "<b>Scaling ensures that the dor products are kept within a range that allows the softmax funtion to operate effectively, providing a more balanced distribution of attention weights and improving the overall learning process of the model.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"scaling.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"scaling.png\", width=500) \n",
    "\n",
    "# here we only showed for the word, wiht respect to other words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Applying Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"apply_softmax.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"apply_softmax.png\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Weighted sum of values\n",
    "    \n",
    "    We multiply the attention weights by corresponding value vecotrs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"weighted_sum.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"weighted_sum.png\", width=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"summary_self_attention.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image  \n",
    "# get the image \n",
    "Image(url=\"summary_self_attention.png\", width=500) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
