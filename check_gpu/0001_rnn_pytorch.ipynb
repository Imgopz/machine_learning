{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the IMDB dataset (same as in TensorFlow implementation)\n",
    "max_features = 10000  # vocabulary size\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a max length\n",
    "max_len = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors and move to device (GPU/CPU)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader for training and testing datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN Model using PyTorch\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]  # Get the output of the last time step\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss, and optimizer\n",
    "model = SimpleRNN(max_features, 128, 128, 1).to(device)  # Move model to device\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.4959, Train Accuracy: 0.7413\n",
      "Validation Loss: 0.7085, Validation Accuracy: 0.6246\n",
      "Epoch 2/10\n",
      "Train Loss: 0.5015, Train Accuracy: 0.7334\n",
      "Validation Loss: 0.6928, Validation Accuracy: 0.6153\n",
      "Epoch 3/10\n",
      "Train Loss: 0.5077, Train Accuracy: 0.7323\n",
      "Validation Loss: 0.7149, Validation Accuracy: 0.6272\n",
      "Epoch 4/10\n",
      "Train Loss: 0.4908, Train Accuracy: 0.7439\n",
      "Validation Loss: 0.7071, Validation Accuracy: 0.6347\n",
      "Epoch 5/10\n",
      "Train Loss: 0.4837, Train Accuracy: 0.7495\n",
      "Validation Loss: 0.7005, Validation Accuracy: 0.6378\n",
      "Epoch 6/10\n",
      "Train Loss: 0.4805, Train Accuracy: 0.7510\n",
      "Validation Loss: 0.7000, Validation Accuracy: 0.6326\n",
      "Epoch 7/10\n",
      "Train Loss: 0.4873, Train Accuracy: 0.7458\n",
      "Validation Loss: 0.7007, Validation Accuracy: 0.6315\n",
      "Epoch 8/10\n",
      "Train Loss: 0.4786, Train Accuracy: 0.7513\n",
      "Validation Loss: 0.7007, Validation Accuracy: 0.6316\n",
      "Epoch 9/10\n",
      "Train Loss: 0.4791, Train Accuracy: 0.7531\n",
      "Validation Loss: 0.7051, Validation Accuracy: 0.6334\n",
      "Epoch 10/10\n",
      "Train Loss: 0.4789, Train Accuracy: 0.7482\n",
      "Validation Loss: 0.7021, Validation Accuracy: 0.6328\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = outputs > 0.5\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = outputs > 0.5\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    val_loss = val_loss / len(test_loader)\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'imdb_simple_rnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input text (a review)\n",
    "input_text = \"This movie was amazing! The acting was superb.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text (using the same tokenizer settings as during training)\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts([input_text])  # Fit the tokenizer on the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input text to a sequence of integers\n",
    "input_sequence = tokenizer.texts_to_sequences([input_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequence to the same length as the training data\n",
    "input_padded = pad_sequences(input_sequence, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a PyTorch tensor\n",
    "input_tensor = torch.tensor(input_padded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRNN(\n",
       "  (embedding): Embedding(10000, 128)\n",
       "  (rnn): RNN(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure that the model is in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Negative\n"
     ]
    }
   ],
   "source": [
    "# Make the prediction (move input tensor to GPU if available)\n",
    "with torch.no_grad():  # No need for gradient calculation during inference\n",
    "    if torch.cuda.is_available():\n",
    "        input_tensor = input_tensor.cuda()\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Get the model's prediction\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Apply the sigmoid function to the output (since we used BCELoss)\n",
    "    prediction = output.squeeze().item()\n",
    "\n",
    "    # Print the predicted sentiment\n",
    "    if prediction > 0.5:\n",
    "        print(\"Prediction: Positive\")\n",
    "    else:\n",
    "        print(\"Prediction: Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
